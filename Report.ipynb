{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Project - Group 67\n",
    "########################################################################################\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This script explains the usage of each of the models\n",
    "##### Each of our models - FNO, Diffusion and VAE, are placed in their respective folders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Prerequisites\n",
    "########################################################################################\n",
    "##### Place all these folders into the HPC\n",
    "##### Have all the necessarily libraries installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. FNO\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commands to run on the HPC terminal:\n",
    "#### Note: The # are explanations as to what the argument is for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with sample training code:\n",
    "###### Note: Visualizations are automatically done during training. Plots are shown in wandb.\n",
    "\n",
    "###### cd FNO\n",
    "###### python main.py  --mode train   \\\n",
    "######                --seq_len 4     \\\n",
    "######                --spatial_resolution 128   \\  # train img resolution\n",
    "######                --batch_size 4   \\  \n",
    "######                --max_epochs 200  \\\n",
    "######                --learning_rate 2e-4   \\\n",
    "######                --loss_function L2   \\ # L2 Loss function, essentially a MSE Loss, could also use H1 Loss which is just a weighted combination of L2 and gradient losses\n",
    "######                --n_modes 30,30,8   \\ # number of fourier modes in each dimension, (x, y, z) z being time therefore we advise using same or smaller value then seq_len\n",
    "######                --hidden_channels 96   \\ # Number of hidden channels used in the linear layers of the FNO layers \n",
    "######                --n_layers 4  \\ # number of FNO layers\n",
    "######                --early_stopping \\  \n",
    "######                --patience 20   \\ # early stopping patience\n",
    "######                --num_predictions_to_log 1   \\ # number of inference predictions to log during training\n",
    "######                --enable_inference_image_logging   \\ # enable logging of inference images during training\n",
    "######                --run_inference_after_train   \\ # run inference after training\n",
    "######                --use_wandb   \\ # uses wandb for logging if not set will use tensorboard\n",
    "######                --wandb_project plasma-simulation\n",
    "######                # --timestep_to_show 70 -> want to log a specific prediction timestep during inference on test loader, used 70 for late plasma dynamics\n",
    "######                # --ablation_study -> run ablation study of training multiple models on different seq_len to compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################\n",
    "### Model testing / inference example:\n",
    "###### python main.py --mode inference --checkpoint 'dirpath' --batch_size 1 --seq_len 4 --spatial_resolution 128 --use_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################\n",
    "########################################################################################\n",
    "#### 2. Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with sample training code:\n",
    "###### Note: Loss Visualizations are automatically done during training. Plots are shown in wandb.\n",
    "\n",
    "###### cd Diffusion\n",
    "###### python main.py --mode train --model \"diffusion\" --max_epochs 100 --diffusion_timesteps 1000 --diffusion_sampling_timesteps 1000  --dataset_name \"DiffusionDataset\" --batch_size 4 --spatial_resolution 256 --num_predictions_to_log 1 --diffusion_dim 64 --diffusion_dim_mults \"1,2,4,8\" --diffusion_flash_attn --use_wandb\n",
    "\n",
    "###### Where diffusion_timesteps, diffusion_sampling_timesteps, diffusion_dim, diffusion_dim_mults and diffusion_flash_attn corresponds to the model parameters timesteps, sampling_timesteps, dim, dim_mults and flash_attn respectively. spatial_resolution is an integer representing the width and height of the input and output frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####################################################################################\n",
    "### Model testing / inference example:\n",
    "###### To get sample data for visualization, modify checkpoint_path in python test_diffusion_model.py, then do:\n",
    "###### python test_diffusion_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The files generated can then be visualized by the code in Report/VisualizeDiffusionModelResult.ipynb, which is copied over here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_images(input_tensor, target_tensor, prediction_tensor, n=1):\n",
    "    for i in range(n):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Get images as NumPy arrays\n",
    "        input_img = input_tensor[i][0].cpu().numpy()\n",
    "        target_img = target_tensor[i][0].cpu().numpy()\n",
    "        prediction_img = prediction_tensor[i][0].cpu().numpy()\n",
    "\n",
    "        # 1. Plots\n",
    "        # Input images\n",
    "        im1 = axes[0].imshow(input_img)\n",
    "        axes[0].set_title('Input Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Target images\n",
    "        im2 = axes[1].imshow(target_img)\n",
    "        axes[1].set_title('Target Image')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # Predicted images\n",
    "        im3 = axes[2].imshow(prediction_img)\n",
    "        axes[2].set_title('Predicted Image')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        # 2. Add individual color bars\n",
    "        cbar1 = fig.colorbar(im1, ax=axes[0], orientation='vertical', fraction=0.02, pad=0.04)\n",
    "        cbar1.set_label('Intensity (Input)')\n",
    "\n",
    "        cbar2 = fig.colorbar(im2, ax=axes[1], orientation='vertical', fraction=0.02, pad=0.04)\n",
    "        cbar2.set_label('Intensity (Target)')\n",
    "\n",
    "        cbar3 = fig.colorbar(im3, ax=axes[2], orientation='vertical', fraction=0.02, pad=0.04)\n",
    "        cbar3.set_label('Intensity (Prediction)')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.load(f'x_tensor_test.pt')\n",
    "y = torch.load(f'y_tensor_test.pt')\n",
    "y_hat = torch.load(f'y_hat_tensor_test.pt')\n",
    "\n",
    "# Check if shape is correct\n",
    "print(x.shape) \n",
    "print(y.shape)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "visualize_images(x, y, y_hat, n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################\n",
    "########################################################################################\n",
    "#### 3. VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Need access to the referenced paths in HPC to run the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training code:\n",
    "\n",
    "###### cd VAE/conditional_diffusion/\n",
    "###### python 34_train_multiscale_vae.py \\\n",
    "######\t--data-dir /dtu/blackhole/1b/223803/tcv_data \\\n",
    "######\t--variables n phi \\\n",
    "######\t--epochs 200 \\\n",
    "######\t--batch-size 8 \\\n",
    "######\t--lr 1e-3 \\\n",
    "######\t--kl-weight 1e-5 \\\n",
    "######\t--latent-dim 256 \\\n",
    "######\t--base-channels 32 \\\n",
    "######\t--loss-type elbow \\\n",
    "######\t--output /dtu/blackhole/1b/223803/runs/vae_kl_low \\\n",
    "######\t--use-amp \\\n",
    "######\t--patience 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####################################################################################\n",
    "### Model inference & data visualization:\n",
    "##### To peform inference & data visualization, run the files in the following order:\n",
    "- 1_extract_tcv_data.py\n",
    "- 2_extract_probe_data.py\n",
    "- 33_model_multiscale_vae_elbow.py\n",
    "- 34_train_multiscale_vae.py\n",
    "- 35_inference_multiscale_vae.py\n",
    "- 36_model_temporal_probe_encoder.py\n",
    "- 37_train_probe_reconstruction.py\n",
    "- 38_inference_probe_reconstruction.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
