{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Project - Group 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook breafly explains how to recreate the results achieved from the experiments described in the paper. Each of the three implemented models - FNO, Diffusion Model, VAE - can be located in their respective directories and run inidvidually by using the commands below to reacreate the results.\n",
    "\n",
    "\n",
    "**Things to keep in mind:**\n",
    "\n",
    "The TCV dataset is a large dataset essentially containing 501 512 x 512 images. We pinned memory in the dataloader to cpu and only load the current batch on the GPU. Altough the commands below and the models described in the report are fairly large, we therefore encourage that engough GPU memory is inquired. \n",
    "\n",
    "The used packages for this study are safed in requirements.txt, please install these first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (131280520.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m venv .venv_report\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m venv .venv_report \n",
    "source .venv_report/bin/activate \n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training an FNO\n",
    "\n",
    "Training can be done by using the following command. Please make sure to first 'cd' into the FNO folder before running this script or adjust the command to call python FNO/main.py otherwise. Beware that due to compute limitation, the batch size, model size and or sequence lengths parameters might need to be adjusted. \n",
    "\n",
    "Note: The logged images and metrics from training and inference are initially done with wandb. To safe logs locally do not use the --use_wandb flag and the code will resort to a Tensorboard logger that saves locally to the specified dirpath. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1137130294.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcd FNO\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## cd FNO \n",
    "python main.py  --mode train   \\\n",
    "            --seq_len 4     \\\n",
    "            --spatial_resolution 128   \\  \n",
    "            --batch_size 4   \\  \n",
    "            --max_epochs 200  \\\n",
    "            --learning_rate 2e-4   \\\n",
    "            --loss_function L2   \\ \n",
    "            --n_modes 30,30,8   \\ \n",
    "            --hidden_channels 96   \\ \n",
    "            --n_layers 4  \\ \n",
    "            --early_stopping \\  \n",
    "            --patience 20   \\ \n",
    "            --num_predictions_to_log 1   \\ \n",
    "            --enable_inference_image_logging   \\ \n",
    "            --run_inference_after_train   \\ \n",
    "            --use_wandb   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run inference change the argument of --checkpoint with the path of the trained model and run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1285472681.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython main.py --mode inference --checkpoint 'dirpath' --batch_size 1 --seq_len 4 --spatial_resolution 128 --use_wandb\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python main.py --mode inference --checkpoint 'dirpath' --batch_size 1 --seq_len 4 --spatial_resolution 128 --use_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diffusion\n",
    "\n",
    "Model Training with sample training code. Again beware that the logs and visualizations are automatically done during training, ensure to not use --use_wandb for local tensorboard logging.\n",
    "\n",
    "In here diffusion_timesteps, diffusion_sampling_timesteps, diffusion_dim, diffusion_dim_mults and diffusion_flash_attn corresponds to the model parameters timesteps, sampling_timesteps, dim, dim_mults and flash_attn respectively. Spatial resolution as above refers to the image resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#cd Diffusion\n",
    "python main.py --mode train \\\n",
    " --model \"diffusion\" \\\n",
    " --max_epochs 100 \\\n",
    " --diffusion_timesteps 1000 \\\n",
    " --diffusion_sampling_timesteps 1000  \\\n",
    " --dataset_name \"DiffusionDataset\" \\\n",
    " --batch_size 4 \\\n",
    " --spatial_resolution 256 \\\n",
    " --num_predictions_to_log 1 \\\n",
    " --diffusion_dim 64 \\\n",
    " --diffusion_dim_mults \"1,2,4,8\" \\\n",
    " --diffusion_flash_attn \\\n",
    " --use_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing / inference example:\n",
    "\n",
    "To sample data for visualization, one has to modify the checkpoint_path in the python file test_diffusion_model.py, and after that run the script test_diffusion_model.py \n",
    "This script will then run inference on the provided testloader, saving the predictions in .pt files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python test_diffusion_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "After generating the prediction files with the step above, one can plot the predictions by using the code below. This code snippet can also be found in VisualizeDiffusionModelResult.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "def visualize_images(input_tensor, target_tensor, prediction_tensor, n=1):\n",
    "    for i in range(n):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Get images as NumPy arrays\n",
    "        input_img = input_tensor[i][0].cpu().numpy()\n",
    "        target_img = target_tensor[i][0].cpu().numpy()\n",
    "        prediction_img = prediction_tensor[i][0].cpu().numpy()\n",
    "\n",
    "        # Input images\n",
    "        im1 = axes[0].imshow(input_img)\n",
    "        axes[0].set_title('Input Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Target images\n",
    "        im2 = axes[1].imshow(target_img)\n",
    "        axes[1].set_title('Target Image')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # Predicted images\n",
    "        im3 = axes[2].imshow(prediction_img)\n",
    "        axes[2].set_title('Predicted Image')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        # 2. Add individual color bars\n",
    "        cbar1 = fig.colorbar(im1, ax=axes[0], orientation='vertical', fraction=0.02, pad=0.04)\n",
    "        cbar1.set_label('Intensity (Input)')\n",
    "\n",
    "        cbar2 = fig.colorbar(im2, ax=axes[1], orientation='vertical', fraction=0.02, pad=0.04)\n",
    "        cbar2.set_label('Intensity (Target)')\n",
    "\n",
    "        cbar3 = fig.colorbar(im3, ax=axes[2], orientation='vertical', fraction=0.02, pad=0.04)\n",
    "        cbar3.set_label('Intensity (Prediction)')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(f'x_tensor_test.pt')\n",
    "y = torch.load(f'y_tensor_test.pt')\n",
    "y_hat = torch.load(f'y_hat_tensor_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_images(x, y, y_hat, n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the VAE one has to run the following command.\n",
    "\n",
    "NOTE: The cli arguments of --data-dir and --ouput have to be adjusted first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# cd VAE/conditional_diffusion/\n",
    "python 34_train_multiscale_vae.py \\\n",
    "    --data-dir /dtu/blackhole/1b/223803/tcv_data \\\n",
    "    --variables n phi \\\n",
    "    --epochs 200 \\\n",
    "    --batch-size 8 \\\n",
    "    --lr 1e-3 \\\n",
    "    --kl-weight 1e-5 \\\n",
    "    --latent-dim 256 \\\n",
    "    --base-channels 32 \\\n",
    "    --loss-type elbow \\\n",
    "    --output /dtu/blackhole/1b/223803/runs/vae_kl_low \\\n",
    "    --use-amp \\\n",
    "    --patience 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform inference and plot visualizations, the following files have to be run in this given order:\n",
    "\n",
    "- 1_extract_tcv_data.py\n",
    "- 2_extract_probe_data.py\n",
    "- 33_model_multiscale_vae_elbow.py\n",
    "- 34_train_multiscale_vae.py\n",
    "- 35_inference_multiscale_vae.py\n",
    "- 36_model_temporal_probe_encoder.py\n",
    "- 37_train_probe_reconstruction.py\n",
    "- 38_inference_probe_reconstruction.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
